{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.io as io\n",
    "import re\n",
    "from scipy.signal import resample as scipy_resample\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ephyspy.features import *  # contains available the features\n",
    "from ephyspy.allen_sdk import ephys_extractor\n",
    "from ephyspy.sweeps import EphysSweepSet, EphysSweep\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: 100%|██████████| 100/100 [00:02<00:00, 40.45it/s]\n",
      "rescaling: 100%|██████████| 341/341 [00:01<00:00, 186.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "from scipy.signal import filtfilt, bessel, iirnotch\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def filter_bad_sweeps(\n",
    "    df: DataFrame,\n",
    "    filt_df: DataFrame,\n",
    "    align_by: str or Tuple[str, str],\n",
    "    filter_by: str or Tuple[str, str],\n",
    ") -> DataFrame:\n",
    "    \"\"\"Remove single sweeps (npoints,t,Ut,It) in dataframe as specified in another.\n",
    "\n",
    "    Align two dataframes by a common column (does not neccesarily have to have\n",
    "    the same name, only the same data) and merge them. Then enforce check which\n",
    "    sweeps need to be excluded. Specified as a comma seperated string.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe to be filtered.\n",
    "        filt_df: Dataframe for filtering.\n",
    "        align_by: the same or two different columns that are used to align the\n",
    "            two different dataframes.\n",
    "        filter_by: columns to equate. see b)\n",
    "\n",
    "    Returns:\n",
    "        pd.Series with True where condition is met.\n",
    "    \"\"\"\n",
    "    align_by = (align_by, align_by) if isinstance(align_by, str) else align_by\n",
    "    filter_by = (filter_by, filter_by) if isinstance(filter_by, str) else filter_by\n",
    "    orig_cols = df.columns\n",
    "\n",
    "    if align_by[0] != align_by[1]:\n",
    "        filt_df.rename({align_by[1]: align_by[0]}, inplace=True)\n",
    "    df = pd.merge(df, filt_df, on=align_by[0], how=\"left\")\n",
    "\n",
    "    def clean_It(ts: ndarray, It: Callable, idxs2rm: List) -> Callable:\n",
    "        \"\"\"Remove stimuli corresponding to bad sweeps identified by idxs2rm.\"\"\"\n",
    "        if isinstance(ts, ndarray):\n",
    "            ts = np.vstack([ts] + [ts[0]] * len(idxs2rm))\n",
    "        return np.delete(It(ts), idxs2rm, axis=0)\n",
    "\n",
    "    def sweep_filter(\n",
    "        df: DataFrame, filter_by: str or Tuple[str, str]\n",
    "    ) -> Tuple[ndarray, ndarray, ndarray, ndarray]:\n",
    "        if not isinstance(df[filter_by], str):  # catches nans\n",
    "            return df.npoints, df.t, df.Ut, df.It\n",
    "        else:\n",
    "            rm = [int(x) - 1 for x in df[filter_by].split(\",\")]\n",
    "            keep = [x for x in range(df.Ut.shape[0]) if x not in rm]\n",
    "            return (\n",
    "                df.npoints[keep],\n",
    "                df.t[keep],\n",
    "                df.Ut[keep],\n",
    "                partial(clean_It, It=df.It, idxs2rm=rm),\n",
    "            )\n",
    "\n",
    "    if not df.empty:\n",
    "        df[[\"npoints\", \"t\", \"Ut\", \"It\"]] = df.apply(\n",
    "            lambda x: sweep_filter(x, filter_by[0]), axis=1, result_type=\"expand\"\n",
    "        )\n",
    "        df = df[df[filter_by[0]] != \"all\"].reset_index()[orig_cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def trange2t(t: ndarray) -> ndarray:\n",
    "    \"\"\"Transform array with tranges to time array.\n",
    "\n",
    "    Args:\n",
    "        t: time ranges in the format t=[[t0,tend,dt],...]\n",
    "\n",
    "    Return:\n",
    "        time array of form t=[[t0,t1,...,tN],...].\n",
    "    \"\"\"\n",
    "    assert t.shape[1] == 3, \"trange needs to have format: t=[[t0,tend,dt],...]\"\n",
    "    t = t.copy()\n",
    "    if np.any(np.diff(t[:, 1])) > 0:  # catch different len tranges of ramps\n",
    "        t[:, 1] = t[-1, 1]\n",
    "    return np.vstack([np.arange(*ts) for ts in t])\n",
    "\n",
    "\n",
    "def mat2dicts(mat: Dict) -> List[Dict]:\n",
    "    \"\"\"Takes io.loadmat outputs for ephys data and does some preprocessing.\n",
    "\n",
    "    Args:\n",
    "        mat: output dicts of io.loadmat with squeeze_me=True,\n",
    "            chars_as_strings=True, mat_dtype=False, simplify_cells=True.\n",
    "\n",
    "    Returns:\n",
    "        stack of preprocessed dictionaries.\"\"\"\n",
    "    dcts = []\n",
    "    num_recs = len(re.findall(\"chan[1-9]+\", \",\".join(mat.keys())))\n",
    "    for n in range(num_recs):\n",
    "        head = mat[f\"head{n+1}\"]\n",
    "        chan = mat[f\"chan{n+1}\"]\n",
    "\n",
    "        # manual refmt and clean\n",
    "        chan[\"adc\"] = chan[\"adc\"].T.astype(float)\n",
    "        chan[\"adc\"] = (\n",
    "            chan[\"adc\"] if chan[\"adc\"].ndim > 1 else chan[\"adc\"].reshape(1, -1)\n",
    "        )\n",
    "        chan[\"tim\"] = chan[\"tim\"].astype(float)\n",
    "        chan.pop(\"mrk\")\n",
    "\n",
    "        fname = head.pop(\"source\").pop(\"name\")  # remove source info\n",
    "        head[\"cell\"] = re.findall(r\"[^\\/\\\\]+(?=\\.)\", fname)[0]  # filename -> cellname\n",
    "        keys2pop = [\n",
    "            [\n",
    "                \"tim\",\n",
    "                \"Channeltype\",\n",
    "                \"Patch\",\n",
    "                \"channeltype\",\n",
    "                \"channeltypeFcn\",\n",
    "                \"classifier\",\n",
    "                \"comment\",\n",
    "                \"embeddeddata\",\n",
    "                \"markerclass\",\n",
    "                \"Environment\",\n",
    "                \"mapstructure\",\n",
    "            ],\n",
    "            [\"Func\", \"Labels\", \"MultiInterval\", \"Multiplex\", \"TargetClass\"],\n",
    "        ]\n",
    "        for key in keys2pop[0]:\n",
    "            head.pop(key)\n",
    "        for key in keys2pop[1]:\n",
    "            head[\"adc\"].pop(key)\n",
    "        head.update(head.pop(\"adc\"))\n",
    "        head[\"label\"] = head.pop(\"Group\")[\"Label\"]\n",
    "        head[\"Npoints\"] = (\n",
    "            np.array([head[\"Npoints\"]])\n",
    "            if not isinstance(head[\"Npoints\"], np.ndarray)\n",
    "            else head[\"Npoints\"]\n",
    "        )\n",
    "\n",
    "        head.update(chan)\n",
    "        dcts.append(head)\n",
    "\n",
    "    return dcts\n",
    "\n",
    "\n",
    "def t2trange(t: ndarray) -> ndarray:\n",
    "    \"\"\"Transform time array to array with trange params.\n",
    "\n",
    "    Args:\n",
    "        t: time array of form t=[[t0,t1,...,tN],...].\n",
    "\n",
    "    Return:\n",
    "        time ranges in the format t=[[t0,tend,dt],...]\n",
    "    \"\"\"\n",
    "\n",
    "    def ensure_correct_t_len(t):\n",
    "        t0 = t[:, 0]\n",
    "        tN = t[:, -1]\n",
    "        dt = t[:, 1] - t0\n",
    "\n",
    "        l_b4 = t.shape[1]\n",
    "        l_after = len(np.arange(t0[0], tN[0], dt[0]))\n",
    "        tN -= (l_after - l_b4) * dt\n",
    "        return t0, tN, dt\n",
    "\n",
    "    assert t.shape[1] != 3, \"t needs to have format: t=[[t0,t1,...,tN],...]\"\n",
    "    t0, tN, dt = ensure_correct_t_len(t)\n",
    "    return np.vstack([t0, tN, dt]).T\n",
    "\n",
    "\n",
    "def get_regex_filter(col: str, regex: str, case_sensitive=False) -> Callable:\n",
    "    \"\"\"return filter for regexs in particular column of a dataframe.\n",
    "\n",
    "    If regex is found, the value at the index is True, else False.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe for filtering.\n",
    "        col: col name to filter.\n",
    "        regex: regular expression that is matched for values in the column.\n",
    "\n",
    "    Returns:\n",
    "        Callable that takes df and returns Series that contains True/False if regex is matched in col.\n",
    "    \"\"\"\n",
    "    match_regex = lambda x: x.str.contains(regex, regex=True)\n",
    "    if case_sensitive:\n",
    "        return lambda df: match_regex(df[col])\n",
    "    else:\n",
    "        return lambda df: match_regex(df[col].str.lower())\n",
    "\n",
    "\n",
    "def import_mats(fpaths: List[str], filter_func: Optional[Callable] = None) -> DataFrame:\n",
    "    r\"\"\"Import and reformat .mat files with electrophysiological data.\n",
    "\n",
    "    Imports all files in given in fpaths. Each .mat file must at least contain\n",
    "    keys 'chan1' & 'head1' to be amenable to this function.\n",
    "\n",
    "    The information that is contained within 'head[i]' and 'chan[i]' is parsed\n",
    "    and a dataframe is constructed that contains the header and data for each\n",
    "    recording.\n",
    "\n",
    "    This includes (pruned to contain only useful entries):\n",
    "    cell: filename / name of the cell\n",
    "    channel: channel of the recording\n",
    "    label: type of recording (fp = firing pattern)\n",
    "    dc: offset current\n",
    "    npoints: number of timepoints\n",
    "    t: timepoints\n",
    "    Ut: voltage traces for different stimuli\n",
    "    It: different stimuli\n",
    "\n",
    "    The function takes care of rescaling the data in t, U and matches the stimuli\n",
    "    to the voltage traces (assuming Ut roughly flat -> I=0, and increasing/decreasing\n",
    "    I in 20pA steps around that). It also selects only traces that have labels\n",
    "    containing 'fp', 'sAP' or 'ramp'.\n",
    "\n",
    "    To save memory the output is returned as:\n",
    "    t = [[t0,tend,dt],...] assuming equal spacing of timebins. (num_sweeps, 3)\n",
    "    Ut with shape (num_sweeps, $N_t$)\n",
    "    It as function that returns array of shape (num_sweeps, $N_t$) for every t\n",
    "    Instead of as:\n",
    "    t = (num_sweeps,$N_t$)\n",
    "    Ut = (num_sweeps, $N_t$)\n",
    "    It = (num_sweeps, $N_t$)\n",
    "    Then `unzip` can be used to obtain arrays of the same shapes.\n",
    "\n",
    "    Args:\n",
    "        fpaths: list of paths to matfiles to import.\n",
    "        filter_func: applies filter to imported mats. Can save memory to get rid\n",
    "            of unwanted recordings already during import.\n",
    "\n",
    "    Returns:\n",
    "        header_df: dataframe containing header information and data for the\n",
    "            electrophysiological recordings.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for fpath in tqdm(fpaths, desc=\"importing\"):\n",
    "        mat = io.loadmat(\n",
    "            fpath,\n",
    "            squeeze_me=True,\n",
    "            chars_as_strings=True,\n",
    "            mat_dtype=False,\n",
    "            simplify_cells=True,\n",
    "        )\n",
    "        dcts = mat2dicts(mat)\n",
    "        df = pd.DataFrame(dcts)\n",
    "\n",
    "        # pre-filter (i.e. remove Imons, EPSPs, EPSCs) -> more memory efficient in for loop, faster outside!\n",
    "        unused_data_filter = get_regex_filter(\"label\", \"epsp|epsc|spont\")\n",
    "        filtered_df = filter_df(df, unused_data_filter(df), True)\n",
    "        if filter_func is not None:\n",
    "            pre_filter = (\n",
    "                (lambda df: pd.Series([False] * len(df)))\n",
    "                if filter_func is None\n",
    "                else filter_func\n",
    "            )\n",
    "            filtered_df = filter_df(filtered_df, pre_filter)\n",
    "\n",
    "        dfs.append(filtered_df)\n",
    "\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    enough_sweeps = df[\"adc\"].apply(lambda x: x.shape[0] > 2)\n",
    "    df = filter_df(df, enough_sweeps)\n",
    "\n",
    "    # rescale\n",
    "    def rescale_Ut(y, unit, lims):\n",
    "        unit = 1 if unit in [\"mV\", \"pA\"] else 1000\n",
    "        xmin, xmax = lims[0] * unit, lims[1] * unit\n",
    "        ymin, ymax = np.min(y), np.max(y)\n",
    "        return (xmax - xmin) / (ymax - ymin) * (y - ymin) + xmin\n",
    "\n",
    "    def rescale_t(t, dt_in, npoints):\n",
    "        tN = (np.diff(t) * 1e-6).reshape(-1, 1)\n",
    "        t0 = np.zeros_like(tN)\n",
    "        dt = dt_in * np.ones_like(tN) * 1000\n",
    "        l_t = [len(np.arange(t0_i, tN_i, dt_i)) for t0_i, tN_i, dt_i in zip(t0, tN, dt)]\n",
    "        l_t = np.array(l_t).reshape(-1, 1)\n",
    "        dl_t = l_t - npoints.reshape(-1, 1)\n",
    "        tN -= dl_t * (dt - 1e-10)  # somehow prevents numerical errors\n",
    "        trange = np.hstack([t0, tN, dt])\n",
    "        return trange\n",
    "\n",
    "    def rescale(x):\n",
    "        rescaled_Ut = rescale_Ut(x.adc, x.Units, x.YLim)\n",
    "        dt = np.prod(x.SampleInterval)\n",
    "        rescaled_t = rescale_t(x.tim, dt, x.Npoints)\n",
    "        return rescaled_t, rescaled_Ut\n",
    "\n",
    "    # TODO: Decide how to handle signals of different length! (i.e. ramp)\n",
    "    # TODO: signals -> 0 to nans according to npoints\n",
    "    tqdm.pandas(desc=\"rescaling\")\n",
    "    df[[\"tim\", \"adc\"]] = df.progress_apply(rescale, axis=1, result_type=\"expand\")\n",
    "\n",
    "    # rename cols and remove obsolete cols\n",
    "    df.rename(\n",
    "        columns={\"adc\": \"Ut\", \"tim\": \"t\", \"DC\": \"dc\", \"Npoints\": \"npoints\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    df = df[[\"cell\", \"dc\", \"channel\", \"label\", \"npoints\", \"t\", \"Ut\"]]\n",
    "\n",
    "    # stimuli\n",
    "    def vectorize_stim_inputs(*args):\n",
    "        is_number = lambda x: isinstance(x, float) or isinstance(x, int)\n",
    "        as_array = lambda x: np.array(x) if is_number(x) else x\n",
    "\n",
    "        def get_reshaped_array(x):\n",
    "            x = as_array(x)\n",
    "            if x.ndim == 0:\n",
    "                return x.reshape(1, 1)\n",
    "            elif x.ndim == 1:\n",
    "                return x.reshape(-1, 1)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        args = [get_reshaped_array(a) for a in args]\n",
    "        dims = [x.shape[0] if x.ndim > 0 else 0 for x in args]\n",
    "\n",
    "        for i, (a, d) in enumerate(zip(args, dims)):\n",
    "            if a.shape[0] != max(dims):\n",
    "                a = np.vstack([a] * max(dims))\n",
    "            args[i] = a\n",
    "        return args\n",
    "\n",
    "    def I_square(t, I, t_on, t_off):\n",
    "        t, I, t_on, t_off = vectorize_stim_inputs(t, I, t_on, t_off)\n",
    "        is_on = np.logical_and(t_on <= t, t <= t_off).astype(float)\n",
    "        return I * is_on\n",
    "\n",
    "    def I_ramp(t, t_on=100, t_off=1100):\n",
    "        t, t_on, t_off = vectorize_stim_inputs(t, t_on, t_off)\n",
    "\n",
    "        is_on = np.logical_and(t_on <= t, t <= t_off).astype(float)\n",
    "        return 25 / 1000 * (t - t_on) * is_on\n",
    "\n",
    "    def I_sAP(t, I=2000, t_on=100, t_off=102):\n",
    "        return I_square(t, I, t_on, t_off)\n",
    "\n",
    "    # add stimulus\n",
    "    def add_stimuli(U, label, npoints):\n",
    "        U = U if U.ndim > 1 else U.reshape(1, -1)\n",
    "        num_sweeps = U.shape[0]\n",
    "        stim_types = {\n",
    "            35000: (200, 1200),\n",
    "            28000: (200, 1200),\n",
    "            20000: (200, 800),\n",
    "            25000: (100, 700),\n",
    "        }\n",
    "        if \"ramp\" in label.lower():\n",
    "            stim = stim_types[npoints[0]]\n",
    "            t_off_0 = (\n",
    "                1200 if \"Ramp\" in label else 1100\n",
    "            )  # capitalized stimuli start later for some reason\n",
    "            t_offs = np.cumsum([t_off_0] + [500] * (num_sweeps - 1))\n",
    "            stimulus = partial(I_ramp, t_on=200, t_off=t_offs)\n",
    "            return stimulus\n",
    "\n",
    "        elif \"fp\" in label.lower():\n",
    "            stim = stim_types[npoints[0]]\n",
    "            I0_idx = np.argmin(np.var(U, axis=1))\n",
    "            I = np.arange(-20 * I0_idx, 20 * (num_sweeps - I0_idx), 20).reshape(-1, 1)\n",
    "            stimulus = partial(I_square, I=I, t_on=stim[0], t_off=stim[1])\n",
    "            return stimulus\n",
    "\n",
    "        elif \"ap\" in label.lower():\n",
    "            t_on = (\n",
    "                200 if \"AP\" in label or \"Cell\" in label else 100\n",
    "            )  # capitalized stimuli start later for some reason\n",
    "            t_off = t_on + 2\n",
    "            stimulus = partial(\n",
    "                I_sAP, I=2000 * np.ones(num_sweeps), t_on=t_on, t_off=t_off\n",
    "            )\n",
    "            return stimulus\n",
    "\n",
    "    df[\"It\"] = df.apply(lambda x: add_stimuli(x.Ut, x.label, x.npoints), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df_filter(\n",
    "    filt_df: DataFrame,\n",
    "    align_by: str or Tuple[str, str],\n",
    "    filter_by: str or Tuple[str, str],\n",
    "    val: Any = None,\n",
    ") -> Series:\n",
    "    \"\"\"filter one dataframe by the column of another.\n",
    "\n",
    "    Align two dataframes by a common column (does not neccesarily have to have\n",
    "    the same name, only the same data) and merge them. Then enforce that either:\n",
    "    a) True if rows contain a specified value\n",
    "    b) True if values in two different columns specified in filter by are equal.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe to be filtered.\n",
    "        filt_df: Dataframe for filtering.\n",
    "        align_by: the same or two different columns that are used to align the\n",
    "            two different dataframes.\n",
    "        filter_by: columns to equate. see b)\n",
    "        val: value to enforce. see a)\n",
    "\n",
    "    Returns:\n",
    "        Callable that takes df and returns Series which is True where condition is met.\n",
    "    \"\"\"\n",
    "    align_by = (align_by, align_by) if isinstance(align_by, str) else align_by\n",
    "    filter_by = (filter_by, filter_by) if isinstance(filter_by, str) else filter_by\n",
    "\n",
    "    if align_by[0] != align_by[1]:\n",
    "        filt_df.rename({align_by[1]: align_by[0]}, inplace=True)\n",
    "\n",
    "    def df_filter(df):\n",
    "        i_prev = df[align_by[0]]\n",
    "        df = pd.merge(df, filt_df, on=align_by[0], how=\"left\")\n",
    "        i_now = df[align_by[0]]\n",
    "\n",
    "        assert len(i_now) == len(i_prev), \"Check for duplicate entries\"\n",
    "        assert all(i_now == i_prev), \"Incorrect alignment of indices\"\n",
    "        if val is None:\n",
    "            return df[filter_by[0]] == df[filter_by[1]]\n",
    "        else:\n",
    "            return df[filter_by[0]] == val\n",
    "\n",
    "    return df_filter\n",
    "\n",
    "\n",
    "def get_std_signal_filter(trange, ut, filt_cutoff=1, notch=None):\n",
    "    \"\"\"convenience function to configure basic ephys signal filter.\"\"\"\n",
    "    dt = trange[0, -1]\n",
    "    b, a = bessel(4, filt_cutoff, fs=1 / dt)\n",
    "    ut = filtfilt(b, a, ut)\n",
    "    if notch is not None:\n",
    "        b, a = iirnotch(notch, 1.5, fs=1e3 / dt)\n",
    "        ut = filtfilt(b, a, ut)\n",
    "    return ut\n",
    "\n",
    "\n",
    "def filter_df(\n",
    "    df: DataFrame,\n",
    "    filter_mask: Series or Callable,\n",
    "    negate: bool = False,\n",
    "    reset_index: bool = True,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Convenience function to apply filter to dataframe.\n",
    "\n",
    "    Applies df.loc[filter_mask] or df.loc[~filter_mask]\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe for filtering.\n",
    "        filter_mask: filter in the form of a boolean pd.Series.\n",
    "        negate: Wether to include or exclude row if filter == True.\n",
    "        reset_index: Whether to reset the index after filtering.\n",
    "\n",
    "    Returns:\n",
    "        filtered dataframe.\n",
    "    \"\"\"\n",
    "    if isinstance(filter_mask, Callable):\n",
    "        filter_mask = filter_mask(df)\n",
    "    reindex = lambda df: df.reset_index(drop=True) if reset_index else df\n",
    "    if negate:\n",
    "        return reindex(df.loc[~filter_mask])\n",
    "    else:\n",
    "        return reindex(df.loc[filter_mask])\n",
    "\n",
    "\n",
    "def preprocess_fp_data(\n",
    "    fp_data: DataFrame,\n",
    "    metadata: Optional[DataFrame] = None,\n",
    "    filt_cutoff: int = 1,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Apply preprocessing steps to dataframe with firing pattern data.\n",
    "\n",
    "    Part of the ephys pipeline. Applies varies hardcoded filters to the fp data.\n",
    "\n",
    "    Args:\n",
    "        fp_data: dataframe holding the firing pattern data.\n",
    "        metadata: dataframe holding the metadata.\n",
    "        filt_cutoff: 4 pole bessel filter cutoff freq in kHz.\n",
    "\n",
    "    Returns:\n",
    "        preprocessed fp data.\n",
    "    \"\"\"\n",
    "\n",
    "    if metadata is not None:\n",
    "        # fp_data = filter_df(\n",
    "        #     fp_data, get_df_filter(metadata, \"cell\", \"Subgroup\", val=\"Adult\")\n",
    "        # )\n",
    "        fp_data = filter_df(\n",
    "            fp_data, get_df_filter(metadata, \"cell\", (\"channel\", \"firing pattern\"))\n",
    "        )\n",
    "        fp_data = filter_df(\n",
    "            fp_data,\n",
    "            get_df_filter(metadata, \"cell\", \"delete\", val=\"Delete\"),\n",
    "            negate=True,\n",
    "        )\n",
    "        fp_data = filter_bad_sweeps(fp_data, metadata, \"cell\", \"fp_rm_sweep\")\n",
    "\n",
    "    fp_data = filter_df(fp_data, fp_data[\"npoints\"].apply(lambda x: len(x) > 5))\n",
    "\n",
    "    if filt_cutoff is not None and len(fp_data) > 0:\n",
    "        fp_filter = lambda df: get_std_signal_filter(df.t, df.Ut, filt_cutoff)\n",
    "        fp_data[\"Ut\"] = fp_data.apply(fp_filter, axis=1)\n",
    "    return fp_data\n",
    "\n",
    "\n",
    "ROOT = \"../smartseq3/data/\"\n",
    "PATH_batch1 = ROOT + \"ephys_batch1/Ephys Mat Data\"\n",
    "PATH_batch2 = ROOT + \"ephys_batch2/Ephys Mat Data\"\n",
    "\n",
    "# metadata\n",
    "metadata_batch1 = pd.read_excel(\n",
    "    ROOT + \"ephys_batch1/cell_metadata_batch1.xlsx\", dtype={\"fp_rm_sweep\": str}\n",
    ")\n",
    "metadata_batch2 = pd.read_excel(\n",
    "    ROOT + \"ephys_batch2/cell_metadata_batch2.xlsx\", dtype={\"fp_rm_sweep\": str}\n",
    ")\n",
    "cols = [\n",
    "    \"cell\",\n",
    "    \"delete\",\n",
    "    \"delete reason\",\n",
    "    \"single AP\",\n",
    "    \"RAMP_AP\",\n",
    "    \"firing pattern\",\n",
    "    \"fp_rm_sweep\",\n",
    "]\n",
    "metadata = pd.concat([metadata_batch1[cols + [\"Subgroup\"]], metadata_batch2[cols]])\n",
    "\n",
    "# file paths\n",
    "fpaths_batch1 = [\n",
    "    os.path.join(PATH_batch1, f) for f in os.listdir(PATH_batch1) if \".mat\" in f\n",
    "]\n",
    "fpaths_batch2 = [\n",
    "    os.path.join(PATH_batch2, f) for f in os.listdir(PATH_batch2) if \".mat\" in f\n",
    "]\n",
    "fpaths = fpaths_batch1 + fpaths_batch2\n",
    "\n",
    "\n",
    "data = import_mats(fpaths[:100])\n",
    "fp_data = filter_df(data, get_regex_filter(\"label\", \"fp\"))\n",
    "fp_data = preprocess_fp_data(fp_data, metadata, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "data must be a EphysSweepSetFeatureExtractor object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_sweepset \u001b[39m=\u001b[39m EphysSweepSet(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     t_set,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     u_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     dc_offset\u001b[39m=\u001b[39mdc,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m test_sweepset\u001b[39m.\u001b[39madd_features(available_spike_features())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_sweepset\u001b[39m.\u001b[39;49madd_features(available_sweepset_features())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_sweepset\u001b[39m.\u001b[39mget_features()\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/sweeps.py:255\u001b[0m, in \u001b[0;36mEphysSweepSet.add_features\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_spike_feature(ft\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, ft)\n\u001b[1;32m    254\u001b[0m \u001b[39melif\u001b[39;00m is_sweepset_feature(ft):  \u001b[39m# needs to be checked b4 sweep feature\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     feature \u001b[39m=\u001b[39m ft(\u001b[39mself\u001b[39;49m, compute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mupdate({feature\u001b[39m.\u001b[39mname: feature})\n\u001b[1;32m    257\u001b[0m \u001b[39melif\u001b[39;00m is_sweep_feature(ft):\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/features/base.py:497\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, data, compute, store_diagnostics, return_value)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_init(data)\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m--> 497\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_value(\n\u001b[1;32m    498\u001b[0m         recompute\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    499\u001b[0m         store_diagnostics\u001b[39m=\u001b[39mstore_diagnostics,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m return_value:\n\u001b[1;32m    502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/features/base.py:456\u001b[0m, in \u001b[0;36m_data_init\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    455\u001b[0m         data, EphysSweepSet\n\u001b[0;32m--> 456\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mdata must be a EphysSweepSet object\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    458\u001b[0m     \u001b[39mfor\u001b[39;00m ft \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset:\n",
      "\u001b[0;31mAssertionError\u001b[0m: data must be a EphysSweepSetFeatureExtractor object"
     ]
    }
   ],
   "source": [
    "t_set, u_set, i_set, dc = fp_data[[\"t\", \"Ut\", \"It\", \"dc\"]].iloc[0]\n",
    "t_set = trange2t(t_set)\n",
    "i_set = i_set(t_set)\n",
    "t_set *= 1e-3\n",
    "\n",
    "# create sweepset\n",
    "test_sweepset = EphysSweepSet(\n",
    "    t_set,\n",
    "    u_set,\n",
    "    i_set,\n",
    "    filter=5,\n",
    "    dc_offset=dc,\n",
    ")\n",
    "\n",
    "test_sweepset.add_features(available_spike_features())\n",
    "test_sweepset.add_features(available_sweepset_features())\n",
    "test_sweepset.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ephyspy.plot import plot_spike_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_features() missing 1 required positional argument: 'fts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_sweepset[\u001b[39m20\u001b[39;49m]\u001b[39m.\u001b[39;49mplot_features()\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_features() missing 1 required positional argument: 'fts'"
     ]
    }
   ],
   "source": [
    "test_sweepset[20].plot_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcts = []\n",
    "# for i in tqdm(range(250)):\n",
    "#     t_set, u_set, i_set, dc = fp_data[[\"t\", \"Ut\", \"It\", \"dc\"]].iloc[i]\n",
    "#     t_set = trange2t(t_set)\n",
    "#     i_set = i_set(t_set)\n",
    "#     t_set *= 1e-3\n",
    "#     start, end = t_set[0, 0], t_set[0, -1]\n",
    "\n",
    "#     # create sweepset\n",
    "#     test_sweepset = EphysSweepSetFeatureExtractor(\n",
    "#         t_set, u_set, i_set, filter=5, dc_offset=dc\n",
    "#     )\n",
    "\n",
    "#     # add available spike, sweep and sweepset features to sweepsetextractor\n",
    "#     test_sweepset.set_stimulus_amplitude_calculator(get_sweep_stim_amp)\n",
    "#     for ft, ft_func in get_available_spike_features().items():\n",
    "#         test_sweepset.add_spike_feature(ft, ft_func)\n",
    "#     for ft, ft_func in get_available_sweep_features(return_ft_info=False).items():\n",
    "#         test_sweepset.add_sweep_feature(ft, ft_func)\n",
    "#     for ft, ft_func in get_available_sweepset_features(return_ft_info=False).items():\n",
    "#         test_sweepset.add_sweepset_feature(ft, ft_func)\n",
    "\n",
    "#     # process sweepset features\n",
    "#     test_sweepset.process()\n",
    "\n",
    "#     dcts.append(test_sweepset.get_sweepset_features())\n",
    "# df = pd.DataFrame(dcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [15:33, 10.48s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, (t_set, u_set, i_set, dc) in tqdm(fp_data[[\"t\", \"Ut\", \"It\", \"dc\"]].iterrows()):\n",
    "    t_set = trange2t(t_set)\n",
    "    i_set = i_set(t_set)\n",
    "    t_set *= 1e-3\n",
    "\n",
    "    # create sweepset\n",
    "    test_sweepset = EphysSweepSetFeatureExtractor(\n",
    "        t_set,\n",
    "        u_set,\n",
    "        i_set,\n",
    "        filter=5,\n",
    "        dc_offset=dc,\n",
    "    )\n",
    "\n",
    "    test_sweepset.add_features(available_spike_features())\n",
    "    test_sweepset.add_features(available_sweepset_features())\n",
    "    test_sweepset.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "data must be a EphysSweepSetFeatureExtractor object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_sweepset \u001b[39m=\u001b[39m EphysSweepSetFeatureExtractor(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     t_set,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     u_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     dc_offset\u001b[39m=\u001b[39mdc,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m test_sweepset\u001b[39m.\u001b[39madd_features(available_spike_features())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m test_sweepset\u001b[39m.\u001b[39;49madd_features(available_sweepset_features())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_sweep \u001b[39m=\u001b[39m test_sweepset[\u001b[39m11\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jnsbck/Uni/PhD/projects/ephyspy/dev.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m times \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mfeature\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mlookup\u001b[39m\u001b[39m\"\u001b[39m: []}\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/sweeps.py:187\u001b[0m, in \u001b[0;36mEphysSweepSetFeatureExtractor.add_features\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_spike_feature(ft\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, ft)\n\u001b[1;32m    186\u001b[0m \u001b[39melif\u001b[39;00m is_sweepset_feature(ft):  \u001b[39m# needs to be checked b4 sweep feature\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     feature \u001b[39m=\u001b[39m ft(\u001b[39mself\u001b[39;49m, compute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mupdate({feature\u001b[39m.\u001b[39mname: feature})\n\u001b[1;32m    189\u001b[0m \u001b[39melif\u001b[39;00m is_sweep_feature(ft):\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/base.py:481\u001b[0m, in \u001b[0;36mSweepsetFeature.__call__\u001b[0;34m(self, data, compute, store_diagnostics, return_value)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    458\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    459\u001b[0m     data: EphysSweepSetFeatureExtractor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     return_value: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    463\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[SweepsetFeature, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    464\u001b[0m     \u001b[39m\"\"\"Compute the feature for a given dataset.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[39m    Essentially chains together `_data_init` and `get_value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39m        The value of the feature.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_init(data)\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m compute:\n\u001b[1;32m    483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_value(\n\u001b[1;32m    484\u001b[0m             recompute\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    485\u001b[0m             store_diagnostics\u001b[39m=\u001b[39mstore_diagnostics,\n\u001b[1;32m    486\u001b[0m         )\n",
      "File \u001b[0;32m~/Uni/PhD/projects/ephyspy/ephyspy/base.py:440\u001b[0m, in \u001b[0;36mSweepsetFeature._data_init\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    441\u001b[0m         data, EphysSweepSetFeatureExtractor\n\u001b[1;32m    442\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mdata must be a EphysSweepSetFeatureExtractor object\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    444\u001b[0m     \u001b[39mfor\u001b[39;00m ft \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset:\n",
      "\u001b[0;31mAssertionError\u001b[0m: data must be a EphysSweepSetFeatureExtractor object"
     ]
    }
   ],
   "source": [
    "# measure execution time\n",
    "import time\n",
    "\n",
    "t_set, u_set, i_set, dc = fp_data[[\"t\", \"Ut\", \"It\", \"dc\"]].iloc[13]\n",
    "t_set = trange2t(t_set)\n",
    "i_set = i_set(t_set)\n",
    "t_set *= 1e-3\n",
    "start, end = t_set[0, 0], t_set[0, -1]\n",
    "\n",
    "# create sweepset\n",
    "test_sweepset = EphysSweepSetFeatureExtractor(\n",
    "    t_set,\n",
    "    u_set,\n",
    "    i_set,\n",
    "    filter=5,\n",
    "    dc_offset=dc,\n",
    ")\n",
    "test_sweepset.add_features(available_spike_features())\n",
    "test_sweepset.add_features(available_sweepset_features())\n",
    "\n",
    "test_sweep = test_sweepset[11]\n",
    "\n",
    "times = {\"time\": [], \"feature\": [], \"lookup\": []}\n",
    "\n",
    "n = len(test_sweepset)\n",
    "for ft_name, FT in available_sweep_features().items():\n",
    "    if ft_name in available_sweepset_features():\n",
    "        SweepFT = available_sweepset_features()[ft_name]\n",
    "        t0 = time.time()\n",
    "        FT()(test_sweep)\n",
    "        t1 = time.time()\n",
    "        t2 = time.time()\n",
    "        FT()(test_sweep)\n",
    "        t3 = time.time()\n",
    "\n",
    "        test_sweep.features = {}\n",
    "\n",
    "        t3 = time.time()\n",
    "        SweepFT(test_sweepset)\n",
    "        t4 = time.time()\n",
    "\n",
    "        t5 = time.time()\n",
    "        SweepFT(test_sweepset)\n",
    "        t6 = time.time()\n",
    "\n",
    "        t1_nolook = t1 - t0\n",
    "        t1_look = t3 - t2\n",
    "        tn_nolook = t4 - t3\n",
    "        tn_look = t6 - t5\n",
    "\n",
    "        # print(f\"1x w.o. lookup vs 1x w. lookup: {t1_nolook:.5f} vs {t1_look:.5f}\")\n",
    "        # print(\n",
    "        #     f\"sweepset w.o. lookup vs sweepset w. lookup: {tn_nolook:.5f} vs {tn_look:.5f}\"\n",
    "        # )\n",
    "        times[\"feature\"] += 2 * [ft_name]\n",
    "        times[\"time\"] += [t1_nolook, t1_look]\n",
    "        times[\"lookup\"] += [False, True]\n",
    "\n",
    "        # print(f\"w.o. lookup {n}x vs sweepset: {n * t1_nolook:.5f} vs {tn_nolook:.5f}\")\n",
    "        # print(f\"w. lookup {n}x vs sweepset: {n * t1_look:.5f} vs {tn_look:.5f}\")\n",
    "\n",
    "        # assert t1_nolook > t1_look, f\"1x w.o. lookup > 1x w. lookup\"\n",
    "        # assert tn_nolook > tn_look, f\"sweepset w.o. lookup > sweepset w. lookup\"\n",
    "\n",
    "        # assert (\n",
    "        #     abs(n * t1_nolook - tn_nolook) / tn_nolook < 0.5\n",
    "        # ), f\"{n}x w.o. lookup > sweepset w.o. lookup\"\n",
    "        # assert (\n",
    "        #     abs(n * t1_look - tn_look) / tn_look < 0.5\n",
    "        # ), f\"{n}x w. lookup > sweepset w. lookup\"\n",
    "\n",
    "times = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.sort_values(by=\"time\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=times, x=\"time\", hue=\"lookup\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FT in fetch_available_fts():\n",
    "    ft_name = FT.__name__.lower()\n",
    "    if not (any(w in ft_name for w in [\"sweepset\", \"apfeature\", \"rheobase\", \"dfdi\"])):\n",
    "        ft = FT(test_sweep)\n",
    "        print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class recomputed_features:\n",
    "#     def __init__(self, feature, recompute=True):\n",
    "#         self.feature = feature\n",
    "#         self.recompute = recompute\n",
    "#         self.without_recompute = self.feature.lookup_sweep_feature\n",
    "\n",
    "#     def __enter__(self):\n",
    "#         def toggle_recompute(func):\n",
    "#             def wrapped_func(*args, **kwargs):\n",
    "#                 kwargs[\"recompute\"] = self.recompute\n",
    "#                 return func(*args, **kwargs)\n",
    "#             return wrapped_func\n",
    "\n",
    "\n",
    "#         self.feature.data.process_spikes()\n",
    "#         self.feature.lookup_sweep_feature = toggle_recompute(\n",
    "#             self.feature.lookup_sweep_feature\n",
    "#         )\n",
    "\n",
    "#         return self.feature\n",
    "\n",
    "#     def __exit__(self, exc_type, exc_value, traceback):\n",
    "#         self.feature.lookup_sweep_feature = self.without_recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_set, u_set, i_set, dc = fp_data[[\"t\", \"Ut\", \"It\", \"dc\"]].iloc[248]\n",
    "t_set = trange2t(t_set)\n",
    "i_set = i_set(t_set)\n",
    "t_set *= 1e-3\n",
    "start, end = t_set[0, 0], t_set[0, -1]\n",
    "\n",
    "# create sweepset\n",
    "test_sweepset = EphysSweepSetFeatureExtractor(\n",
    "    t_set, u_set, i_set, filter=5, dc_offset=dc\n",
    ")\n",
    "\n",
    "# add available spike, sweep and sweepset features to sweepsetextractor\n",
    "test_sweepset.set_stimulus_amplitude_calculator(get_sweep_stim_amp)\n",
    "for ft, ft_func in get_available_spike_features().items():\n",
    "    test_sweepset.add_spike_feature(ft, ft_func)\n",
    "for ft, ft_func in get_available_sweep_features(return_ft_info=False).items():\n",
    "    test_sweepset.add_sweep_feature(ft, ft_func)\n",
    "for ft, ft_func in get_available_sweepset_features(return_ft_info=False).items():\n",
    "    test_sweepset.add_sweepset_feature(ft, ft_func)\n",
    "\n",
    "# process sweepset features\n",
    "test_sweepset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(12, 12))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell.process(\"long_squares\")  # needs to be run before spikes\n",
    "# cell.process(\"long_squares_spiking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell.long_squares_features(\"spiking\").sweep_features(\"avg_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_df = fp_sweepset.get_sweep_features()  # .applymap(strip_info)\n",
    "# sweepset_fts = {\n",
    "#     ft: ft_func(ft_df) for ft, ft_func in ef.get_fp_sweepset_ft_dict().items()\n",
    "# }\n",
    "# sweepset_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01c410baa9fc88dfee5f8d965682dff580a05acc8a6315cbf16020b8fcc70d20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
